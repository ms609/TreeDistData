---
title: "§6. Cluster recovery tests"
author: "Martin R. Smith <martin.smith@durham.ac.uk>"
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2:
    toc: no
  rmarkdown::html_vignette:
    default: yes
bibliography: ../inst/REFERENCES.bib
csl: ../inst/apa-old-doi-prefix.csl
vignette: >
  %\VignetteIndexEntry{Lin et al. cluster recovery tests}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


Distance measurements can be used to identify clusters of similar trees.  I used three approaches to generate clusters of similar trees, and tested each metric in its ability to recover these clusters [@Lin2012].

```{r, echo=FALSE}
library('TreeDistData')

# Obtain values from TreeDistData documentation
nTrees <- 50
nLeaves <- 40
replicates <- 500L

data(linTestOneResults, package='TreeDistData')
data(linTestTwoResults, package='TreeDistData')
data(linTestSPRResults, package='TreeDistData')

withText <- 2.4
yRange <- c(0, 1)
Panel <- function (letter, x, ...) {
  text(x, 1.05, paste0('(', letters[letter], ')'), pos=2, offset=0, ...)
}
```

For the first test, I generated `r replicates` datasets of `r nTrees` binary
trees with _n_ = `r nLeaves` leaves.
Each set of trees was created by uniformly sampling two k-leaf ‘skeleton’ trees,
where k ranges from 0.3 n to 0.9 _n_. 
From each skeleton, `r nTrees` trees were generated by adding each of the remaining
_n – k_ leaves at a uniformly selected point on the tree.

```{R, echo=FALSE, fig.asp=1.23}
PlotFunc <- function () {
  par(mfrow=c(2, 2), mar = c(withText, withText, 1, 0), xpd = NA,
      mgp = c(withText - 1L, 0.5, 0), cex=0.8)

  x <- seq(30, 70, 10)
  plot(type='n', range(x) - c(5, 0), yRange, axes=FALSE,
       ylab='Success rate', xlab='% leaves in skeleton')
  axis(1, x)
  yLab <- seq(0, 1, length.out = 5)
  axis(2, yLab)
  scores <- colMeans(linTestOneResults) / replicates
  methods <- rownames(scores)
  XX <- lapply(methods, function (method) {
    lines(x, scores[method, ], col = TreeDistCol(method), lwd = 2)
  })
  Panel(1, min(x))

  par(mar = c(withText, 1, 1, 0))

  x <- seq(10, 40, 10) / 100 * nLeaves
  plot(type='n', range(x), yRange, axes=FALSE,
       ylab='', xlab='Leaf label interchanges')
  axis(1, x)
  axis(2, at = yLab, labels = rep('', 5))
  scores <- colMeans(linTestTwoResults) / replicates
  methods <- rownames(scores)
  XX <- lapply(methods, function (method) {
    lines(x, scores[method, ], col = TreeDistCol(method), lwd = 2)
  })

  Panel(2, min(x))


  par(mar = c(withText, withText, 1, 0), xpd = NA)
  x <- seq(20, 100, 20) / 100 * nLeaves
  plot(type='n', range(x), yRange, axes=FALSE,
       ylab='Success rate', xlab='Subtree prune and regraft operations')
  axis(1, x)
  yLab <- seq(0, 1, length.out = 5)
  axis(2, yLab)
  scores <- colMeans(linTestSPRResults) / replicates
  XX <- lapply(rownames(scores), function (method) {
    lines(x, scores[method, ], col = TreeDistCol(method), lwd = 2)
  })
  Panel(3, min(x))

  par(mar = c(0, 0, 0, 0))
  plot.new()
  legend('center', tdAbbrevs[methods], lty = 1, col = TreeDistCol(methods),
         bty='n', lwd = 2, cex = 0.9)
}

PlotFunc()
```

Ranking: rank for each value of _k_ and each clustering method, using lowest
value in ties.  Then average across all values of _k_ and all clustering
methods for each test.

```{r ranking, echo=FALSE}
x <- seq(30, 70, 10)
tm <- 'average'
ranks <- cbind(
  rowMeans(apply(-linTestOneResults, c(1, 3), rank, ties.method = tm)),
  rowMeans(apply(-linTestTwoResults, c(1, 3), rank, ties.method = tm)),
  rowMeans(apply(-linTestSPRResults, c(1, 3), rank, ties.method = tm)))

dimnames(ranks) <- list(tdAbbrevs[rownames(ranks)],
                        paste('Test', c('one', 'two', 'three')))
ranks <- cbind(ranks, 'Mean rank' = round(rowMeans(ranks), 2))
ranks <- ranks[order(ranks[, 'Mean rank']), ]

knitr::kable(ranks)

```


<!--PlotMe(4, oneCol, 1.23, PlotFunc)-->

## References
